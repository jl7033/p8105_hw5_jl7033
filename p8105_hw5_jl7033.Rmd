---
title: "P8105 - Homework 5"
author: "Joe LaRocca"
date: "2024-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Problem 1

#### Create the Birthday Simulator Function

```{r}

bday_sim = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  duplicate = length(unique(bdays)) < n
  return(duplicate)
}

```

#### Run 10,000 Iterations from Sample Size 2 to 50

```{r}

sim_res =
  expand_grid(
    n = c(2:50),
    iter = 1:10000
  ) |>
  mutate(res = map_lgl(n, bday_sim)) |>
  group_by(n) |>
  summarize(prob = mean(res))

```

#### Make a Plot of Sample Size vs. Probability

```{r}

sim_res |>
  ggplot(aes(x = n, y = prob)) + 
  geom_point() + 
  labs(
    title = "Birthday Problem: Plot of Sample Size vs. Probability of a Duplicate Birthday",
    x = "Sample Size",
    y = "Probability of Duplicate"
  )

```

The plot of sample size vs. probability of a duplicate is S-shaped, and strongly resembles a Logistic curve. The probability of a duplicate first crosses 50\% at $n = 23$.

## Problem 2

#### Create a t_test function that cleans the test and selects for the data we want

```{r}

t_test = function(mu, n = 30, sd = 5){
  test = t.test(rnorm(n, mu, sd)) |>
    broom::tidy() |>
    select(estimate, p.value)
  return(test)
}

```

#### Run the simulation 5000 times for mu between 0 and 6 inclusive

```{r}

sim_res = expand_grid(
  mu = c(0:6),
  iter = c(1:5000)
) %>%
  mutate(res = map(mu, t_test))

```

#### Format the data for plotting

```{r}

sim_res = sim_res |>
  unnest(res) |>
  mutate(reject = p.value < 0.05)

```

#### Make Plot #1

```{r}

sim_res |>
  select(mu, reject) |>
  group_by(mu) |>
  summarize(prob_reject = mean(reject)) |>
  ggplot(aes(x = mu, y = prob_reject)) +
  geom_point() + 
  labs(
    x = "Mu value",
    y = "Probability of Rejecting the Null Hypothesis at Alpha = 0.05",
    title = "Probability of Rejecting the Null Hypothesis at Alpha = 0.05, n = 30, SD = 5"
  )

```

From this plot, we can see that there is a strong, positive, and nonlinear association between effect size and power. More specifically, we see the sharpest increases in power between $\mu = 1$ and $\mu = 2$ and between $\mu = 2$ and $\mu = 3$. The probability of rejection first crosses 50\% at $\mu = 2$.

#### Make Plot #2

```{r}

sim_res_all = sim_res |>
  group_by(mu) |>
  summarize(mean = mean(estimate)) |>
  mutate(type = "all")

sim_res_reject = sim_res |>
  group_by(mu, reject) |>
  summarize(mean = mean(estimate)) |>
  filter(reject == TRUE) |>
  mutate(type = "reject") |>
  select(mu, mean, type)

sim_res_total = rbind(sim_res_all, sim_res_reject)

```

```{r}

sim_res_total |>
  ggplot(aes(x = mu, y = mean, col = type)) + 
  geom_point(size = 2, alpha = 0.6) + 
  labs(
    x = "Mu value",
    y = "Mean Mu_hat value",
    title = "Mean value of Mu_hat for 5,000 Random Samples, n = 30, SD = 5"
  )

```

In theory, the sample averages for each value of $\mu$ are approximately equal to the true values of $\mu$ regardless of whether the null hypothesis is rejected or not, because we know $E(X) = \mu$ and $\bar{x} = \frac{\sum_{k = 1}^n}{n}$. So, $E(\hat{\mu}) = E(\frac{\sum_{i = 1}^n x_i}{n}) = \frac{1}{n} E(\sum_{i = 1}^n x_i) = \frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n)) = \frac{1}{n}({n\mu}) = \mu$. In other words, the expectation of the sample mean is equal to the expectation of each individual data point, which is $\mu$. Therefore, the "red" data points (counting all samples, not just the ones in which the null hypothesis was rejected) approximate the line $y = x$.

We can see that when the probability of rejection is much less than 100\%, the mean $\hat{\mu}$ value is higher than the $\mu$ value for the samples in which the null was rejected. First, we can see that the mean $\hat{mu}$ for samples in which the null is rejected when $\mu = 0$ is close to 0, since some samples were rejected because the value of $\hat{mu}$ was significantly **less than** 0. For $\mu$ values of 1 or larger, the probability of $\hat{mu}$ being significantly less than 0 is minuscule, so the mean $\hat{mu}$ among the samples in which the null hypothesis was rejected starts to more closely approximate the mean $\hat{\mu}$ for all samples. When the probability of rejection approaches 100\%, the mean $\hat{\mu}$ value in the samples for which the null is rejected closely approaches the mean $\hat{\mu}$ value for all samples (mostly because the probability of rejection is very high).


This is because rejection of the null hypothesis happens only when $\hat{\mu}$ is far enough away from 0. 

## Problem 3

#### Upload the Data

```{r}

homicides = read_csv("data/homicide-data.csv")

```

#### Describing the Dataset

```{r}

summary(homicides$reported_date)

```

```{r}

mean(as.numeric(homicides$victim_age), na.rm = TRUE)
median(as.numeric(homicides$victim_age), na.rm = TRUE)

```

```{r}

homicides |>
  group_by(victim_race) |>
  count() |>
  summarize(prop = n/nrow(homicides)) 

```

The raw data encompasses a total of 50 large cities throughout the United States. The data spans an eight-year period from January 2007 to November 2015 (I found this by running `summary(homicides$reported_date`). Not accounting for homicides for which the age of the victim was unknown, the mean victim age was 31.8 and the median age was 28. Nearly 64\% of the victims were Black; the next highest represented racial group was Hispanic, which accounted for 13.2\% of the homicides in the dataset.

#### Combine city and state into a "city_state" column

```{r}

homicides = homicides |> 
  unite(col = "city_state", city, state, sep = ", ")

```

#### Summarize to find Number of Total / Unsolved Homicides by City

```{r}

homicides = homicides |>
  mutate(unsolved = 
      disposition == "Closed without arrest" |
      disposition == "Open/No arrest")

homicide_stats = homicides |> 
  group_by(city_state) |>
  summarize(
    num_homicides = length(disposition),
    num_unsolved = sum(unsolved)
  )

homicide_stats

```

#### Create a prop_test function that tests proportion of unsolved homicides by city

```{r}

prop_test_interval = function(city, p = 0.5) {
  x = homicide_stats |>
    filter(city_state == city) |>
    pull(num_unsolved)
  
  n = homicide_stats |> 
    filter(city_state == city) |>
    pull(num_homicides)
  
  test = prop.test(x, n, p) |>
    broom::tidy() |>
    select(estimate, conf.low, conf.high)
    return(test)
}

```

#### Find confidence interval for Baltimore

```{r}

homicide_stats |>
  filter(city_state == "Baltimore, MD") |>
  mutate(results = map(city_state, prop_test_interval)) |>
  unnest(results)

```

#### Find confidence interval for all cities

```{r}

homicide_cis = homicide_stats |>
  group_by(city_state) |>
  mutate(results = map(city_state, prop_test_interval)) |>
  unnest(results) |>
  filter(city_state != "Tulsa, AL")

homicide_cis

```

#### Make Plot of Estimates and CIs

```{r}

homicide_cis |>
  ggplot(aes(x = fct_reorder(city_state, estimate), y = estimate)) + 
  geom_boxplot() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  coord_flip() + 
   labs(
    x = "Esimated Proportion of Unsolved Homicides",
    y = "City",
    title = "Estimations of Proportion of Unsolved Homicides and CIs by U.S. City"
  )

```





