---
title: "P8105 - Homework 5"
author: "Joe LaRocca"
date: "2024-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Problem 1

#### Create the Birthday Simulator Function

```{r}

bday_sim = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  duplicate = length(unique(bdays)) < n
  return(duplicate)
}

```

#### Run 10,000 Iterations from Sample Size 2 to 50

```{r}

sim_res =
  expand_grid(
    n = c(2:50),
    iter = 1:10000
  ) |>
  mutate(res = map_lgl(n, bday_sim)) |>
  group_by(n) |>
  summarize(prob = mean(res))

```

#### Make a Plot of Sample Size vs. Probability

```{r}

sim_res |>
  ggplot(aes(x = n, y = prob)) + 
  geom_point() + 
  labs(
    title = "Birthday Problem: Plot of Sample Size vs. Probability of a Duplicate Birthday",
    x = "Sample Size",
    y = "Probability of Duplicate"
  )

```

The plot of sample size vs. probability of a duplicate is S-shaped, and strongly resembles a Logistic curve. The probability of a duplicate first crosses 50\% at $n = 23$.

## Problem 2

#### Create a t_test function that cleans the test and selects for the data we want

```{r}

t_test = function(mu, n = 30, sd = 5){
  test = t.test(rnorm(n, mu, sd)) |>
    broom::tidy() |>
    select(estimate, p.value)
  return(test)
}

```

#### Run the simulation 5000 times for mu between 0 and 6 inclusive

```{r}

sim_res = expand_grid(
  mu = c(0:6),
  iter = c(1:5000)
) %>%
  mutate(res = map(mu, t_test))

```

#### Format the data for plotting

```{r}

sim_res = sim_res |>
  unnest(res) |>
  mutate(reject = p.value < 0.05)

```

#### Make Plot #1

```{r}

sim_res |>
  select(mu, reject) |>
  group_by(mu) |>
  summarize(prob_reject = mean(reject)) |>
  ggplot(aes(x = mu, y = prob_reject)) +
  geom_point()

```

From this plot, we can see that there is a strong, positive, and nonlinear association between effect size and power. More specifically, we see the sharpest increases in power between $\mu = 1$ and $\mu = 2$ and between $\mu = 2$ and $\mu = 3$. The probability of rejection first crosses 50\% at $\mu = 2$.

#### Make Plot #2

```{r}

sim_res |>
  ggplot(aes(x = mu, y = estimate, col = reject)) +
  geom_point(alpha = 0.1)

```

From the plot, we can see that the sample averages for each value of $\mu$ are approximately equal to the true values of $\mu$. In theory, this would be true because we know $E(X) = \mu$ and $\bar{x} = \frac{\sum_{k = 1}^n}{n}$. So, $E(\bar{x}) = E(\frac{\sum_{i = 1}^n x_i}{n}) = \frac{1}{n} E(\sum_{i = 1}^n x_i) = \frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n)) = \frac{1}{n}({n\mu}) = \mu$. In other words, the expectation of the sample mean is equal to the expectation of each individual data point, which is $\mu$.

## Problem 3

#### Upload the Data

```{r}

homicides = read_csv("data/homicide-data.csv")

```

#### Describing the Dataset

```{r}

summary(homicides$reported_date)

```

```{r}

mean(as.numeric(homicides$victim_age), na.rm = TRUE)
median(as.numeric(homicides$victim_age), na.rm = TRUE)

```

```{r}

homicides |>
  group_by(victim_race) |>
  count() |>
  summarize(prop = n/nrow(homicides)) 

```

The raw data encompasses a total of 50 large cities throughout the United States. The data spans an eight-year period from January 2007 to November 2015 (I found this by running `summary(homicides$reported_date`). Not accounting for homicides for which the age of the victim was unknown, the mean victim age was 31.8 and the median age was 28. Nearly 64\% of the victims were Black; the next highest represented racial group was Hispanic, which accounted for 13.2\% of the homicides in the dataset.

#### Combine city and state into a "city_state" column

```{r}

homicides = homicides |> 
  unite(col = "city_state", city, state, sep = ", ")

```

#### Summarize to find Number of Total / Unsolved Homicides by City

```{r}

homicides = homicides |>
  mutate(unsolved = 
      disposition == "Closed without arrest" |
      disposition == "Open/No arrest")

homicides |> 
  group_by(city_state) |>
  summarize(
    num_homicides = length(disposition),
    num_unsolved = sum(unsolved)
  )

```

#### Using prop.test for Baltimore

```{r}

homicides |>
  filter(city_state == "Baltimore, MD") |>
  pull(unsolved) |>
  prop.test(sum(unsolved), n = length(unsolved), p = 0.5)
  

```

```{r}

x = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE)

prop.test(sum(x), n = 7, p = 0.5)

```

```{r}

heads <- rbinom(1, size = 100, prob = .5)
prop.test(heads, 100)          # continuity correction TRUE by default
prop.test(heads, 100, correct = FALSE)

heads

```

